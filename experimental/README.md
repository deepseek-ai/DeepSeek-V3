# DeepZig V3 Implementation ğŸš€

A high-performance implementation of DeepSeek V3 in [Zig](https://ziglang.org/) for blazingly fast inference.

> **âš ï¸ Status: Experimental Foundation** 
> 
> This project provides an **experimental foundation** for DeepZig V3 with working draft implementation:
> - âœ… **HTTP server** with OpenAI-compatible API
> - âœ… **BLAS-accelerated tensor operations** (Apple Accelerate working)
> - âœ… **Cross-platform build system** (Zig 0.15.0-dev)
> - âœ… **Memory management** and backend architecture
> - âœ… **Apple Silicon detection and optimization**
> - âœ… **Functional matrix operations** (significant performance improvement)
> 
> **Recent Progress**: Matrix operations now use BLAS acceleration<br/>
> **Performance Status**: 1160+ GFLOPS with Apple Accelerate backend working (measured on Apple M1 Macbook)<br/>
> 
> See [Performance Results](#performance-notes) for detailed benchmarks.

## Overview

This experimental implementation aims to leverage Zig's unique advantages for systems programming to create a high-performance LLM inference engine:

- **Zero-cost abstractions** with compile-time optimization
- **Direct hardware access** for SIMD and platform-specific optimizations  
- **Manual memory management** without garbage collection pauses
- **Single binary deployment** with no runtime dependencies
- **Cross-platform compilation** for multiple architectures

**ğŸš€ BLAS Acceleration Achieved!** We've successfully integrated Apple Accelerate backend delivering **1000+ GFLOPS** performance - a **3000x speedup** over the initial naive implementation. Measured on an M1 Macbook.

**ğŸ”— Related**: See the [main project README](../README.md) for architecture overview and vision.

## Project Structure

```
experimental/
â”œâ”€â”€ build.zig              # Build system configuration
â”œâ”€â”€ build.zig.zon          # Package dependencies  
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.zig           # HTTP server entry point
â”‚   â”œâ”€â”€ core/              # Core ML components
â”‚   â”‚   â”œâ”€â”€ root.zig       # Module exports
â”‚   â”‚   â”œâ”€â”€ tensor.zig     # SIMD-optimized tensors
â”‚   â”‚   â”œâ”€â”€ model.zig      # DeepSeek V3 model
â”‚   â”‚   â”œâ”€â”€ attention.zig  # MLA attention mechanism
â”‚   â”‚   â”œâ”€â”€ moe.zig        # Mixture of Experts
â”‚   â”‚   â”œâ”€â”€ tokenizer.zig  # Text tokenization
â”‚   â”‚   â”œâ”€â”€ backend.zig    # Backend abstraction
â”‚   â”‚   â”œâ”€â”€ memory.zig     # Memory management
â”‚   â”‚   â””â”€â”€ math/          # Math utilities
â”‚   â”‚       â”œâ”€â”€ root.zig   # Math module exports
â”‚   â”‚       â”œâ”€â”€ simd.zig   # SIMD operations
â”‚   â”‚       â”œâ”€â”€ activation.zig  # Activation functions
â”‚   â”‚       â””â”€â”€ rms_norm.zig    # RMS normalization
â”‚   â”œâ”€â”€ web/               # HTTP API layer
â”‚   â”‚   â”œâ”€â”€ root.zig       # Web module exports
â”‚   â”‚   â”œâ”€â”€ server.zig     # HTTP server (std.http)
â”‚   â”‚   â”œâ”€â”€ handlers.zig   # Request handlers
â”‚   â”‚   â”œâ”€â”€ middleware.zig # CORS, auth, rate limiting
â”‚   â”‚   â”œâ”€â”€ websocket.zig  # WebSocket support
â”‚   â”‚   â”œâ”€â”€ openai.zig     # OpenAI API compatibility
â”‚   â”‚   â”œâ”€â”€ request.zig    # Request wrapper
â”‚   â”‚   â””â”€â”€ response.zig   # Response wrapper
â”‚   â”œâ”€â”€ backends/          # Compute backends
â”‚   â”‚   â”œâ”€â”€ cpu/           # CPU with SIMD
â”‚   â”‚   â”œâ”€â”€ metal/         # Apple Silicon
â”‚   â”‚   â””â”€â”€ cuda/          # NVIDIA GPUs
â”‚   â””â”€â”€ wasm/
â”‚       â””â”€â”€ main.zig       # WebAssembly entry point
â”œâ”€â”€ bench/
â”‚   â””â”€â”€ main.zig           # Performance benchmarks
â””â”€â”€ README.md               # This file
```

## Requirements

- **Zig 0.15.0-dev**
- Platform-specific requirements:
  - **macOS**: Xcode Command Line Tools (for Metal backend)
  - **Linux**: CUDA Toolkit (for CUDA backend, optional)
  - **Windows**: CUDA Toolkit (for CUDA backend, optional)

## Quick Start

### Building

```bash
# Clone and navigate to experimental directory
cd experimental/

# Build the project
zig build

# Run the server
zig build run

# Run tests
zig build test

# Run benchmarks
zig build bench

# Build WebAssembly
zig build wasm
```

### Running the Server

```bash
# Start server on default port (8080)
./zig-out/bin/deepseek-v3-zig

# Custom configuration
./zig-out/bin/deepseek-v3-zig --port 3000 --backend metal --model ./path/to/model
```

### API Usage

The server exposes OpenAI-compatible endpoints:

```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "deepseek-v3",
    "messages": [{"role": "user", "content": "Hello!"}],
    "max_tokens": 100
  }'

# Health check
curl http://localhost:8080/health

# Model info
curl http://localhost:8080/v1/models
```

## Performance Features

### SIMD Optimizations

- **x86_64**: AVX2/AVX-512 vectorization for matrix operations
- **ARM64**: NEON SIMD for Apple Silicon optimization
- **Auto-vectorization**: Compiler-optimized loops with `@Vector` types

### Backend Support

| Backend | Status | Features |
|---------|--------|----------|
| **CPU** | âœ… Implemented | Multi-threaded, SIMD, cache-optimized |
| **Metal** | ğŸš§ In Progress | Apple Silicon GPU, unified memory |
| **CUDA** | ğŸš§ Planned | NVIDIA GPU, Tensor Cores |
| **WebGPU** | ğŸ“‹ Future | Browser GPU acceleration |

### Memory Management

- **Arena allocators** for request-scoped memory
- **Memory pools** for tensor allocations
- **Zero-copy operations** where possible
- **Cache-friendly** data layouts

## Development Status

### âœ… Drafted
- [x] Project structure and build system
- [x] Core tensor operations with SIMD
- [x] HTTP server with OpenAI API compatibility
- [x] CPU backend with optimizations
- [x] Memory management utilities
- [x] Benchmark suite

### ğŸš§ In Progress
- [ ] DeepSeek V3 model architecture
- [ ] Multi-Head Latent Attention (MLA)
- [ ] Mixture of Experts (MoE) implementation
- [ ] Metal backend for Apple Silicon
- [ ] Model loading and weight management

### ğŸ“‹ Planned
- [ ] CUDA backend for NVIDIA GPUs
- [ ] WebSocket streaming
- [ ] Model quantization (INT8, FP16)
- [ ] Flash Attention optimization
- [ ] Distributed inference
- [ ] Advanced sampling strategies

## Architecture Decisions

### Why Zig?

1. **Performance**: Zero-cost abstractions without runtime overhead
2. **Memory Safety**: Compile-time memory management without GC
3. **Simplicity**: Single binary deployment, cross-compilation
4. **Control**: Direct hardware access for optimization

### Design Principles

- **Modularity**: Clean separation between core, web, and backend layers
- **Performance**: SIMD-first design with cache-friendly algorithms  
- **Compatibility**: OpenAI API compatibility for easy adoption
- **Extensibility**: Plugin architecture for new backends

## Contributing

This is an experimental project! Contributions are welcome:

1. **Core ML**: Implement transformer layers, attention mechanisms
2. **Backends**: Optimize CUDA/Metal compute kernels
3. **Performance**: Profile and optimize bottlenecks
4. **Testing**: Add comprehensive test coverage
5. **Documentation**: Improve setup and usage guides

### Development Setup

```bash
# Install Zig 0.15.0-dev
# https://ziglang.org/download/

# Clone repository
git clone [repository-url]
cd experimental/

# Run tests during development
zig build test --watch

# Format code
zig fmt src/
```

## Benchmarks

Run benchmarks to measure performance:

```bash
zig build bench
```

**Hardware Context**: Benchmarks run on Apple M1 MacBook Pro (MacBookPro17,1) with 16GB unified memory, Zig 0.15.0-dev.703+597dd328e, debug build.

Example output:
```
ğŸš€ DeepZig V3 Performance Benchmarks
==========================================

ğŸ¯ DYNAMIC BENCHMARK SUMMARY
===============================

ğŸ“Š Matrix Multiplication Performance:
  â€¢ 256Ã—256: 0.0 ms, 937 GFLOPS
  â€¢ 512Ã—512: 0.2 ms, 1084 GFLOPS  
  â€¢ 1024Ã—1024: 2.1 ms, 1164 GFLOPS
  â€¢ 2048Ã—2048: 20.9 ms, 823 GFLOPS
  ğŸ† Peak measured: 1164 GFLOPS at 1024Ã—1024

ğŸ§® BLAS Configuration:
  â€¢ Backend: Apple Accelerate
  â€¢ Theoretical peak: 2600 GFLOPS (estimated)

â• Tensor Operations:
  â€¢ SIMD Addition: 3.5 GB/s

ğŸ’¾ Memory Performance:
  â€¢ Copy Bandwidth: 20.9 GB/s
  â€¢ Random Access Latency: 1.8 ns

ğŸ¯ Performance Assessment:
  âœ… Acceptable: BLAS delivering 1000+ GFLOPS
  â€¢ Est. efficiency: 44% (vs theoretical peak)

Note: Benchmarked on Apple M1 MacBook Pro under heavy load 
(should be significantly higher on a clean system).
```

**Performance Results** (Apple M1 MacBook Pro under heavy load):
- **Matrix 256Ã—256**: 0.0ms/iter, **937 GFLOPS**
- **Matrix 512Ã—512**: 0.2ms/iter, **1084 GFLOPS** (peak performance)
- **Matrix 1024Ã—1024**: 2.1ms/iter, **1164 GFLOPS**
- **Matrix 2048Ã—2048**: 20.9ms/iter, **823 GFLOPS**

**Performance Achievement**: From **6418ms naive** â†’ **2.2ms BLAS** = **2900x speedup** on matrix operations

**System Status**:
- âœ… **BLAS Backend**: Apple Accelerate integration delivering acceptable performance
- âœ… **Peak Performance**: **1164 GFLOPS measured** (44% of theoretical maximum, impressive under load)
- âœ… **Memory Bandwidth**: 20.9 GB/s copying, well-optimized operations
- âœ… **Hardware Detection**: M-series Apple Silicon detection functional

## Known Issues

- **Model Loading**: Currently creates dummy models - real weight loading not implemented
- **Tokenizer**: Placeholder implementation - needs proper BPE tokenizer
- **WebSocket**: Basic structure only - streaming not implemented
- **Metal/CUDA**: Backend stubs only - GPU kernels not implemented

## License

This experimental implementation follows the same license as the original DeepSeek V3 project.

## Resources

- [Original DeepSeek V3 Paper](https://arxiv.org/abs/2412.19437)
- [Zig Language Documentation](https://ziglang.org/documentation/master/)
- [Zig Performance Guide](https://github.com/ziglang/zig/wiki/Performance)
- [SIMD in Zig](https://ziglang.org/documentation/master/#Vectors)

## Is This Ready for Production? 

**No** - this is a research/development foundation. But it's **theoretical and compiles**:

- **What works now**: âœ… Compiles and runs with Zig 0.15.0-dev, HTTP server, tensor operations, SIMD math, benchmarks execute successfully
- **What's missing**: Optimized matrix operations, actual DeepSeek V3 model implementation
- **Timeline**: Foundation is **compiling**, model implementation is the next major milestone

## Comparison to Other Projects

| Project | Language | Status | Focus |
|---------|----------|--------|-------|
| **This** | Zig | Foundation + API | Web-first inference |
| llama.cpp | C++ | Production | CLI/library |
| Candle | Rust | Production | ML framework |
| ZML | Zig | Research | Low-level ML ops |

**Unique advantages**: Built-in web server, Zig's zero-cost abstractions, single binary deployment.

---

**âš¡ Built with Zig for blazing fast LLM inference!** 

## Performance Notes

**Current Status**: âœ… **BLAS integration working** - Apple Accelerate backend now functional in draft implementation.

**Performance Results** (Apple M1 MacBook Pro under heavy load):
- **Matrix 256Ã—256**: 0.0ms/iter, **937 GFLOPS**
- **Matrix 512Ã—512**: 0.2ms/iter, **1084 GFLOPS**
- **Matrix 1024Ã—1024**: 2.1ms/iter, **1164 GFLOPS** (peak performance)
- **Matrix 2048Ã—2048**: 20.9ms/iter, **823 GFLOPS**

**Performance Achievement**: From **6418ms naive** â†’ **2.1ms BLAS** = ~**3000x speedup** on matrix operations.

**System Status**:
- âœ… **BLAS Backend**: Apple Accelerate integration working
- âœ… **Peak Performance**: **1164 GFLOPS measured** (44% of theoretical maximum)
- âœ… **Memory Bandwidth**: 20.9 GB/s copying, well-optimized operations
- âœ… **Hardware Detection**: M-series Apple Silicon detection functional

**Next Steps**: Focus on transformer architecture, attention mechanisms, and model-specific optimizations for the draft DeepSeek V3 implementation. 